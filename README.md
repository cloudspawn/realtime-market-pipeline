# realtime-market-pipeline

> âš ï¸ **Work in Progress** â€” Production-grade version of [realtime-crypto-elt](https://github.com/cloudspawn/realtime-crypto-elt)

[![Status](https://img.shields.io/badge/status-in%20development-yellow)]()
[![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python&logoColor=white)](https://python.org)
[![Kafka](https://img.shields.io/badge/Kafka-Confluent-black?logo=apachekafka&logoColor=white)](https://confluent.io)
[![BigQuery](https://img.shields.io/badge/BigQuery-Google%20Cloud-4285F4?logo=googlebigquery&logoColor=white)](https://cloud.google.com/bigquery)
[![dbt](https://img.shields.io/badge/dbt-1.9-FF694B?logo=dbt&logoColor=white)](https://getdbt.com)
[![Airflow](https://img.shields.io/badge/Airflow-2.10-017CEE?logo=apacheairflow&logoColor=white)](https://airflow.apache.org)

Production-grade real-time market data pipeline: multi-source ingestion â†’ Kafka â†’ BigQuery â†’ dbt, with Airflow orchestration and Prometheus/Grafana monitoring.

## Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INGESTION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Binance WebSocket â”€â”€â”                                          â”‚
â”‚  CoinGecko API â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶ Producers â”€â”€â–¶ Kafka (multi-topics)   â”‚
â”‚                      â”‚         â”‚                                â”‚
â”‚                      â”‚    retry + reconnection                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PROCESSING                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kafka â”€â”€â–¶ Consumer â”€â”€â–¶ BigQuery (raw) + GCS (parquet)         â”‚
â”‚                â”‚                                                â”‚
â”‚                â”œâ”€â”€ Batch insert                                 â”‚
â”‚                â”œâ”€â”€ Dead Letter Queue                            â”‚
â”‚                â””â”€â”€ Dual-write (DWH + Data Lake)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRANSFORMATION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Airflow â”€â”€â–¶ dbt run (every 10 min)                            â”‚
â”‚                 â”‚                                               â”‚
â”‚                 â”œâ”€â”€ staging (views)                             â”‚
â”‚                 â”œâ”€â”€ intermediate (views)                        â”‚
â”‚                 â””â”€â”€ marts (tables)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MONITORING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Prometheus â—€â”€â”€ metrics (throughput, latency, errors)          â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  Grafana â”€â”€â–¶ dashboards + alerting                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

### Implemented âœ…
- **Multi-source ingestion**: Binance WebSocket (real-time trades) + CoinGecko API (market data)
- **20 cryptocurrencies**: BTC, ETH, SOL, ADA, DOT, AVAX, LINK, MATIC, XRP, BNB, DOGE, SHIB, LTC, ATOM, NEAR, APT, ARB, OP, INJ, SUI
- **Kafka streaming**: Multi-topic architecture with partitioning by symbol
- **Dual-write consumer**: BigQuery (data warehouse) + GCS Parquet (data lake)
- **Data lake**: Parquet files partitioned by date (`raw/trades/YYYY/MM/DD/`)
- **dbt transformations**: staging â†’ intermediate â†’ marts
- **Airflow orchestration**: DAG with dbt run/test every 10 minutes
- **PostgreSQL**: Production-ready Airflow metadata database
- **Docker Compose**: Full orchestration with one command
- **Production patterns**: Retry with exponential backoff, automatic reconnection, graceful shutdown, Dead Letter Queue
- **Observability**: Prometheus metrics (throughput, errors, connections)
- **Structured logging**: JSON logs for easy parsing

### Coming soon ğŸš§
- Grafana dashboards (public)

## Project Structure
```
realtime-market-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producers/
â”‚   â”‚   â”œâ”€â”€ binance_ws.py       # WebSocket real-time trades
â”‚   â”‚   â””â”€â”€ coingecko.py        # API polling for market data
â”‚   â”œâ”€â”€ consumers/
â”‚   â”‚   â””â”€â”€ bigquery_consumer.py # Dual-write to BigQuery + GCS
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ config.py           # Pydantic settings
â”‚       â”œâ”€â”€ logging.py          # Structured logging
â”‚       â”œâ”€â”€ kafka_client.py     # Kafka producer wrapper
â”‚       â””â”€â”€ metrics.py          # Prometheus metrics
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/            # stg_trades, stg_prices
â”‚       â”œâ”€â”€ intermediate/       # int_trades_aggregated, int_prices_latest
â”‚       â””â”€â”€ marts/              # mart_trading_summary
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Custom Airflow image with dbt
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ dbt_dag.py          # DAG for dbt orchestration
â”œâ”€â”€ docker-compose.yml          # Full orchestration
â”œâ”€â”€ Dockerfile                  # App image for producers/consumer
â””â”€â”€ README.md
```

## Quick Start

### Prerequisites
- Docker & Docker Compose
- Confluent Cloud account (Kafka)
- GCP account (BigQuery, GCS)

### Setup
```bash
# Clone
git clone https://github.com/cloudspawn/realtime-market-pipeline.git
cd realtime-market-pipeline

# Configure
cp .env.example .env
# Edit .env with your credentials

# Add GCP service account key
cp /path/to/your/key.json secrets/gcp-key.json

# Start everything
docker compose up -d
```

### Access

| Service | URL |
|---------|-----|
| Airflow | http://localhost:8080 |
| Prometheus (producer-binance) | http://localhost:8000/metrics |
| Prometheus (consumer) | http://localhost:8001/metrics |
| Prometheus (producer-coingecko) | http://localhost:8002/metrics |

### Logs
```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f producer-binance
docker compose logs -f consumer
docker compose logs -f airflow-scheduler
```

### Stop
```bash
docker compose down
```

## Configuration

See `.env.example` for all available settings.

Required:
- Confluent Cloud credentials (Kafka)
- GCP credentials (BigQuery, GCS)
- Airflow admin credentials
- PostgreSQL password

## Metrics

Prometheus metrics exposed on each service:

| Metric | Type | Description |
|--------|------|-------------|
| `producer_messages_produced_total` | Counter | Messages sent to Kafka |
| `producer_errors_total` | Counter | Producer errors by type |
| `producer_websocket_connections` | Gauge | Active WebSocket connections |
| `consumer_messages_consumed_total` | Counter | Messages consumed from Kafka |
| `consumer_messages_inserted_total` | Counter | Messages inserted into BigQuery |

## License

MIT